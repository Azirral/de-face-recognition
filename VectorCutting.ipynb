{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:24.789417Z",
     "start_time": "2024-10-28T18:58:24.784023Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, os, re, time, logging, cv2, torch\n",
    "from pathlib import Path\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip\n",
    "import datetime\n",
    "from win32com.propsys import propsys, pscon\n",
    "import subprocess\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:24.824631Z",
     "start_time": "2024-10-28T18:58:24.821296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set logging level to DEBUG\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Replace 'video.mp4' with the path to your media file\n",
    "#probe = ffmpeg.probe('//153.19.52.107/emboa/IO3-sessions/NEW STRUCTURE/Camera/GUT/S01/C02/Untitled 140.mp4')\n",
    "#print(probe)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "PYCHARM_DEBUG=True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:24.878615Z",
     "start_time": "2024-10-28T18:58:24.875411Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:24.900681Z",
     "start_time": "2024-10-28T18:58:24.888706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_table(table):\n",
    "    for row in table:\n",
    "        print(row)\n",
    "\n",
    "def create_table(tab1, tab2, tab3):\n",
    "    result_tab = []\n",
    "\n",
    "    emotions = ['Unknown', 'Happy', 'Sad', 'Scared', 'Disgusted', 'Surprised', 'Angry']\n",
    "\n",
    "    for row1, row2, row3 in zip(tab1, tab2, tab3):\n",
    "        sum_rows = [int(a) + int(b) + int(c) for a, b, c in zip(row1[1:], row2[1:], row3[1:])]\n",
    "\n",
    "        total_sum = sum(sum_rows)\n",
    "        if total_sum > 0:\n",
    "            # Calculate the percentage distribution\n",
    "            percentages = [round((x / total_sum) * 100, 2) for x in sum_rows]\n",
    "        else:\n",
    "            percentages = [0] * len(sum_rows)\n",
    "\n",
    "        result_tab.append([row1[0]] + percentages)\n",
    "\n",
    "    return result_tab\n",
    "\n",
    "# Makes table equal\n",
    "def fill_table(table, csvreader):\n",
    "    seconds = 0\n",
    "    stop = 0\n",
    "    for row in csvreader:\n",
    "        row[0] = int(row[0].split('.')[0])\n",
    "        if seconds <= row[0] and stop == 0:  #fill table with missing seconds\n",
    "            for i in range(0, row[0]):\n",
    "                table.append([i, '0', '0', '0', '0', '0', '0', '0'])\n",
    "                seconds += 1\n",
    "            stop = 1\n",
    "            table.append(row)\n",
    "        else:\n",
    "            table.append(row)\n",
    "\n",
    "\n",
    "# Create percentages for every second\n",
    "def csv_files_reader(base_path):\n",
    "    index = ['I', 'II', 'III']\n",
    "    tab1 = []\n",
    "    tab2 = []\n",
    "    tab3 = []\n",
    "\n",
    "    if os.path.exists(base_path):\n",
    "\n",
    "        for i in index:\n",
    "            file = os.path.join(base_path, i)\n",
    "            for file_name in os.listdir(file):\n",
    "                file_path = os.path.join(file, file_name)\n",
    "\n",
    "                with open(file_path, 'r') as file:\n",
    "                    csvreader = csv.reader(file)\n",
    "                    header = next(csvreader)\n",
    "\n",
    "                    if i == 'I':\n",
    "                        fill_table(tab1, csvreader)\n",
    "                    elif i == 'II':\n",
    "                        fill_table(tab2, csvreader)\n",
    "                    elif i == 'III':\n",
    "                        fill_table(tab3, csvreader)\n",
    "    else:\n",
    "        print(f\"File path {base_path} doesn't exist.\")\n",
    "\n",
    "    large_table = max([tab1, tab2, tab3], key=len)\n",
    "\n",
    "    for tab in [tab1, tab2, tab3]:\n",
    "        if len(tab) < len(large_table):\n",
    "            diff = len(large_table) - len(tab)\n",
    "\n",
    "            if len(tab) == 0:\n",
    "                sec = 0\n",
    "                for sec in range(len(large_table)):\n",
    "                    tab.append([sec, '0', '0', '0', '0', '0', '0', '0'])\n",
    "            else:\n",
    "                sec = tab[-1][0]  #latest second in table\n",
    "                for sec in range(tab[-1][0] + 1, len(large_table)):\n",
    "                    tab.append([sec, '0', '0', '0', '0', '0', '0', '0'])\n",
    "\n",
    "    return create_table(tab1, tab2, tab3)\n",
    "\n",
    "\n",
    "def get_boris_vector(path):\n",
    "    table = csv_files_reader(path)\n",
    "    df = pd.DataFrame(table)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "def get_video_metadata(file_path):\n",
    "    try:\n",
    "        # Get the file status\n",
    "        file_stat = os.stat(file_path)\n",
    "\n",
    "        # Get the last modified time\n",
    "        last_modified_time_str = time.ctime(file_stat.st_mtime)\n",
    "        last_modified_timestamp = int(file_stat.st_mtime)\n",
    "\n",
    "        # Get the duration of the video\n",
    "        with VideoFileClip(file_path) as video:\n",
    "            duration = video.duration\n",
    "\n",
    "        metadata = {\n",
    "            'last_modified_time': last_modified_time_str,\n",
    "            'last_modified_timestamp': last_modified_timestamp,\n",
    "            'duration': int(duration),  # Duration in seconds\n",
    "            'initial_timestamp' : last_modified_timestamp - int(duration)\n",
    "        }\n",
    "        return metadata\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return f\"The file {file_path} does not exist.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:24.910735Z",
     "start_time": "2024-10-28T18:58:24.902694Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:24.919879Z",
     "start_time": "2024-10-28T18:58:24.911746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gut_timestamp(video_path):\n",
    "    return get_video_metadata(video_path)['initial_timestamp']"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:24.956490Z",
     "start_time": "2024-10-28T18:58:24.952888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ituyu_timestamp(video_path):\n",
    "    try:\n",
    "        properties = propsys.SHGetPropertyStoreFromParsingName(video_path)\n",
    "        timestamp = int(properties.GetValue(pscon.PKEY_Media_DateEncoded).GetValue().timestamp())\n",
    "        return timestamp\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving video metadata: {e}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:24.991288Z",
     "start_time": "2024-10-28T18:58:24.985013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def maap_timestamp(video_path):\n",
    "    try:\n",
    "        # Use ffprobe to extract video metadata\n",
    "        result = subprocess.run(\n",
    "            ['ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_entries', 'format_tags=creation_time', video_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        metadata = json.loads(result.stdout)\n",
    "        \n",
    "        # Extract the creation time if available\n",
    "        creation_time = metadata.get('format', {}).get('tags', {}).get('creation_time', None)\n",
    "        \n",
    "        if creation_time:\n",
    "            creation_datetime = datetime.fromisoformat(creation_time.replace('Z', '+00:00'))\n",
    "            unix_timestamp = int(creation_datetime.timestamp())\n",
    "            return unix_timestamp\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving video metadata: {e}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:25.041486Z",
     "start_time": "2024-10-28T18:58:25.036299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_video_recording_timestamp(video_path, research_center):\n",
    "    timestamp = 0\n",
    "    if research_center == 'GUT':\n",
    "        timestamp = gut_timestamp(video_path)\n",
    "    elif research_center == 'ITU-YU':\n",
    "        timestamp = ituyu_timestamp(video_path)\n",
    "    elif research_center == 'MAAP':\n",
    "        timestamp = maap_timestamp(video_path)\n",
    "    return timestamp"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the moment of the start of the vector and its frequency\n",
    "def get_unix_and_hz(file_path):\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        unix = int(float(next(reader)[0]))  # Convert the first cell to integer\n",
    "        hz = int(float(next(reader)[0]))  # Convert the second cell to integer\n",
    "    return unix, hz"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:25.070788Z",
     "start_time": "2024-10-28T18:58:25.065505Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "# Trim the vector to be the multiple of its frequency\n",
    "def trim_vector(vector, rate):\n",
    "    length = len(vector)\n",
    "    if length % rate != 0:\n",
    "        # Calculate how many elements need to be removed\n",
    "        excess_elements = length % rate\n",
    "        # Trim the vector\n",
    "        vector = vector[:-excess_elements]\n",
    "    return vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:25.114297Z",
     "start_time": "2024-10-28T18:58:25.109800Z"
    }
   },
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": [
    "# Change the frequency by averaging the values\n",
    "def mean_of_values(vector, rate):\n",
    "    # Ensure the length of the vector is a multiple of n\n",
    "    if len(vector) % rate != 0:\n",
    "        raise ValueError(\"Length of the vector must be a multiple of frequency\")\n",
    "\n",
    "    # Reshape the vector into a 2D array where each row is a group of n elements\n",
    "    reshaped_vector = np.reshape(vector, (-1, rate))\n",
    "\n",
    "    # Calculate the mean along the rows\n",
    "    mean_values = np.mean(reshaped_vector, axis=1)\n",
    "\n",
    "    return mean_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:25.131533Z",
     "start_time": "2024-10-28T18:58:25.120307Z"
    }
   },
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:25.159301Z",
     "start_time": "2024-10-28T18:58:25.143543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_frame_embeddings(path):\n",
    "    # Step 1: Initialize FaceNet model and MTCNN detector\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mtcnn = MTCNN(keep_all=True, device=device)  # MTCNN for face detection\n",
    "    facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)  # Pre-trained FaceNet model\n",
    "\n",
    "    # Step 2: Load video file and get frame rate\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "    else:\n",
    "        print(\"Video opened successfully!\")\n",
    "\n",
    "    # Calculate frames per second (fps) and total frames\n",
    "    frame_interval = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second of the video\n",
    "\n",
    "    # Step 3: Process video at 1-second intervals\n",
    "    frame_count = 0\n",
    "    frame_embeddings = []\n",
    "    num_of_features = 512\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Check if the frame is at the 1-second interval\n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert frame to RGB (OpenCV uses BGR by default)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_pil = Image.fromarray(frame_rgb)\n",
    "            try:\n",
    "                # Step 4: Detect faces in the frame\n",
    "                boxes, _ = mtcnn.detect(frame_pil)\n",
    "\n",
    "                if boxes is not None and len(boxes) > 0:\n",
    "                    # Step 5: Find the smallest box based on area\n",
    "                    areas = [(box[2] - box[0]) * (box[3] - box[1]) for box in boxes]\n",
    "                    smallest_box = boxes[np.argmin(areas)]\n",
    "\n",
    "                    x1, y1, x2, y2 = [int(coord) for coord in smallest_box]\n",
    "                    cropped_face = frame_rgb[y1:y2, x1:x2]\n",
    "                    cropped_face_pil = Image.fromarray(cropped_face)\n",
    "\n",
    "                    # Step 6: Crop the face from the frame\n",
    "                    # Detect face in the cropped image and get embeddings\n",
    "                    faces = mtcnn(cropped_face_pil)\n",
    "\n",
    "                    if faces is not None and len(faces) > 0:\n",
    "                        faces = faces.to(device)\n",
    "                        embeddings = facenet(faces)  # Generate embeddings\n",
    "                        frame_embeddings.append(embeddings.detach().cpu().numpy())\n",
    "                        print(\"Successful.\")\n",
    "                    else:\n",
    "                        # Append zeros if no valid face tensor detected\n",
    "                        frame_embeddings.append(np.zeros(num_of_features, dtype=np.float32))\n",
    "                        print(\"Failed after cropping.\")\n",
    "                else:\n",
    "                    # Append zeros if no face detected in this frame\n",
    "                    frame_embeddings.append(np.zeros(num_of_features, dtype=np.float32))\n",
    "                    print(\"No face detected in this frame.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle any exception raised by MTCNN and append zeros\n",
    "                print(f\"Exception during face detection: {e}\")\n",
    "                frame_embeddings.append(np.zeros(num_of_features, dtype=np.float32))\n",
    "\n",
    "        frame_count += 1\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Step 7: Release video capture\n",
    "    cap.release()\n",
    "    return pd.DataFrame(flatten_to_512(frame_embeddings))"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:25.215864Z",
     "start_time": "2024-10-28T18:58:25.208633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten_to_512(L):\n",
    "    \"\"\"\n",
    "    This function flattens a nested structure of arrays, ensuring that\n",
    "    each element in the final result is an array of exactly 512 elements.\n",
    "    \n",
    "    It also reshapes arrays of shape (1, 512) to (512,).\n",
    "    \n",
    "    :param L: List of arrays or nested arrays.\n",
    "    :return: A list of arrays where each array has exactly 512 elements.\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "\n",
    "    def flatten_recursive(item):\n",
    "        \"\"\"\n",
    "        Helper function to recursively flatten nested arrays.\n",
    "        \"\"\"\n",
    "        # If item is a numpy array\n",
    "        if isinstance(item, np.ndarray):\n",
    "            # Check if the array has shape (512,)\n",
    "            if item.shape == (512,):\n",
    "                result.append(item)\n",
    "            # Check if the array has shape (1, 512), reshape it to (512,)\n",
    "            elif item.shape == (1, 512):\n",
    "                result.append(item.reshape(512,))\n",
    "            else:\n",
    "                raise ValueError(f\"Array with incorrect size found: {item.shape}. Expected shapes: (512,) or (1, 512).\")\n",
    "\n",
    "        elif isinstance(item, list):\n",
    "            # If it's a list, recurse into its elements\n",
    "            for sub_item in item:\n",
    "                flatten_recursive(sub_item)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported type encountered: {type(item)}. Expected list or numpy array.\")\n",
    "\n",
    "    # Start the recursive flattening process\n",
    "    flatten_recursive(L)\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T18:58:25.246867Z",
     "start_time": "2024-10-28T18:58:25.232875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def slice_vectors(video_path, biosignal_path, input_storage_path, boris, boris_storage_path, research_center):\n",
    "    # Get the path for EDA, TEMP and HR\n",
    "    EDA_path = os.path.join(biosignal_path, 'EDA.csv')\n",
    "    TEMP_path = os.path.join(biosignal_path, 'TEMP.csv')\n",
    "    HR_path = os.path.join(biosignal_path, 'HR.csv')\n",
    "    \n",
    "    # Get only the data\n",
    "    EDA = pd.read_csv(EDA_path, skiprows = 2, header = None)\n",
    "    TEMP = pd.read_csv(TEMP_path, skiprows = 2, header = None)\n",
    "    HR = pd.read_csv(HR_path, skiprows = 2, header = None)\n",
    "    frame_embeddings = extract_frame_embeddings(video_path)\n",
    "    \n",
    "    # Get the starting time and frequency\n",
    "    unix_EDA, hz_EDA = get_unix_and_hz(EDA_path)\n",
    "    unix_TEMP, hz_TEMP = get_unix_and_hz(TEMP_path)\n",
    "    unix_HR, hz_HR = get_unix_and_hz(HR_path)\n",
    "    unix_video = get_video_recording_timestamp(video_path, research_center)\n",
    "\n",
    "    # Unificate the frequencies  \n",
    "    EDA_mean = mean_of_values(trim_vector(EDA, hz_EDA), hz_EDA)\n",
    "    TEMP_mean = mean_of_values(trim_vector(TEMP, hz_TEMP), hz_TEMP)\n",
    "    HR_mean = mean_of_values(trim_vector(HR, hz_HR), hz_HR)\n",
    "\n",
    "    # Get the lengths\n",
    "    length_EDA = len(EDA_mean)\n",
    "    length_TEMP = len(TEMP_mean)\n",
    "    length_HR = len(HR_mean)\n",
    "    length_video = get_video_metadata(video_path)['duration']\n",
    "    \n",
    "    # Get the vectors of starts and ends for biosignals and video\n",
    "    starts = [unix_HR, unix_TEMP, unix_EDA, unix_video]\n",
    "    ends = [unix_HR + length_HR, unix_TEMP + length_TEMP, unix_EDA + length_EDA, unix_video + length_video]\n",
    "\n",
    "    # Get the latest start of any vector\n",
    "    last_start = max(starts)\n",
    "    \n",
    "    # Get the earliest end of any vector\n",
    "    first_end = min(ends)\n",
    "\n",
    "    # Get matching indexes for start and end for every vector\n",
    "    EDA_first_index = last_start - unix_EDA\n",
    "    EDA_last_index = first_end - unix_EDA\n",
    "    TEMP_first_index = last_start - unix_TEMP\n",
    "    TEMP_last_index = first_end - unix_TEMP\n",
    "    HR_first_index = last_start - unix_HR\n",
    "    HR_last_index = first_end - unix_HR\n",
    "    video_first_index = last_start - unix_video\n",
    "    video_last_index = first_end - unix_video\n",
    "\n",
    "    print('EDA: ', EDA_last_index - EDA_first_index)\n",
    "    print('TEMP: ', TEMP_last_index - TEMP_first_index)\n",
    "    print('HR: ', HR_last_index - HR_first_index)\n",
    "    print('video: ', video_last_index - video_first_index)\n",
    "    \n",
    "    # Slice the biosignals based on the index\n",
    "    sliced_EDA = EDA[EDA_first_index:EDA_last_index]\n",
    "    sliced_TEMP = TEMP[TEMP_first_index:TEMP_last_index]\n",
    "    sliced_HR = HR[HR_first_index:HR_last_index]\n",
    "    sliced_boris = boris[video_first_index:video_last_index].copy() # to surpass the warning of working on a view and not a copy\n",
    "    sliced_frame_embeddings = frame_embeddings[video_first_index:video_last_index]\n",
    "    \n",
    "    # Create one df with all biosignals\n",
    "    input = pd.concat([sliced_EDA.reset_index(drop=True), sliced_TEMP.reset_index(drop=True), sliced_HR.reset_index(drop=True), sliced_frame_embeddings.reset_index()], axis=1)\n",
    "    \n",
    "    # Rename column names\n",
    "    new_columns = ['EDA', 'TEMP', 'HR'] + input.columns[3:].tolist()\n",
    "    input.columns = new_columns\n",
    "    input = input.drop(input.columns[3], axis=1)\n",
    "    # Drop index and contempt from BORIS\n",
    "    sliced_boris.drop([sliced_boris.columns[0], sliced_boris.columns[1]], axis=1, inplace=True)\n",
    "    sliced_boris.columns = ['Happy', 'Sad', 'Scared', 'Disgusted', 'Surprised', 'Angry']\n",
    "    # Add the biosignals to file\n",
    "    input.to_csv(input_storage_path, index=False, sep=',')\n",
    "    sliced_boris.to_csv(boris_storage_path, index=False, sep=',')"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def traverse():\n",
    "    root_dir = '//153.19.52.107/emboa/IO3-sessions/NEW STRUCTURE'\n",
    "    target_root_dir = '//153.19.52.107/emboa/IO3-sessions/NEW STRUCTURE/de-earlyfusionthesis'\n",
    "    boris_ext = 'Analysis/BORIS/'\n",
    "    type = ['Camera', 'Wristband']\n",
    "    #research_centers = ['GUT']\n",
    "    research_centers = ['GUT', 'ITU-YU', 'MAAP']\n",
    "    #s_values = [\"S01\"]\n",
    "    s_values = [\"S01\", \"S02\", \"S03\", \"S04\", \"S05\", \"S06\", \"S07\", \"S08\", \"S09\", \"S10\", \"S11\" ]\n",
    "    #c_values = [\"C02\"]\n",
    "    c_values = [\"C01\", \"C02\", \"C03\", \"C04\", \"C05\", \"C06\", \"C07\", \"C08\", \"C09\", \"C10\", \"C11\", \"C12\", \"C13\" ]\n",
    "    for research_center in research_centers:\n",
    "        for session in s_values:\n",
    "            # Create the session path for biosignal and camera + BORIS\n",
    "            camera_path = Path(root_dir).joinpath(type[0], research_center, session)\n",
    "            signals_path = Path(root_dir).joinpath(type[1], research_center, session)\n",
    "            if camera_path.is_dir() and signals_path.is_dir():\n",
    "                # Create the meeting path for biosignal and camera + BORIS\n",
    "                for camera in c_values:\n",
    "                    exact_camera_path = Path(camera_path).joinpath(camera)\n",
    "                    exact_signals_path = Path(signals_path).joinpath(camera)\n",
    "                    if exact_camera_path.is_dir() and exact_signals_path.is_dir():\n",
    "                        boris_path = Path(exact_camera_path).joinpath(boris_ext)\n",
    "                        if boris_path.is_dir():\n",
    "                            # Quick fix of two MAAP sessions being divided TODO: Fix this\n",
    "                            if not(research_center == 'MAAP' and ((session == 'S01' and camera == 'C05') or (session == 'S03' and camera == 'C05'))):\n",
    "                                mp4_pattern = []\n",
    "                                if research_center == 'GUT':\n",
    "                                    mp4_pattern.append(re.compile(r'^Untitled \\d+\\.mp4$'))\n",
    "                                elif research_center == 'ITU-YU':\n",
    "                                    mp4_pattern.append(re.compile(r'^ITU-C\\d{2}-S\\d{2}-\\d{8}-Camera\\.mp4$'))\n",
    "                                elif research_center == 'MAAP':\n",
    "                                    mp4_pattern.append(re.compile(r'^MAAP-C\\d{2}-S\\d{2}-\\d{8}-Camera\\.AVI$'))\n",
    "                                    mp4_pattern.append(re.compile(r'^MAAP-C\\d{2}-S\\d{2}-\\d{8}-Camera\\.MOV$'))\n",
    "                                for files_in_camera_dir in os.listdir(exact_camera_path):\n",
    "                                    if any(pattern.match(files_in_camera_dir) for pattern in mp4_pattern):\n",
    "                                        video_path = os.path.join(exact_camera_path, files_in_camera_dir)\n",
    "                                # TODO: TRY EXCEPT FOR not matching pattern, skip iteration\n",
    "                                filename = research_center + '_' + session + '_' + camera\n",
    "                                boris_filename = filename + '_BORIS.csv'\n",
    "                                boris_target_dir = Path(target_root_dir).joinpath(research_center, boris_filename)\n",
    "                                \n",
    "                                input_filename = filename + '_input.csv'\n",
    "                                input_target_dir = Path(target_root_dir).joinpath(research_center, input_filename)\n",
    "                                # Create combined BORIS vector\n",
    "                                boris = get_boris_vector(boris_path)\n",
    "                                # Create one vector of biosignals, sliced BORIS and sliced video\n",
    "                                slice_vectors(video_path, exact_signals_path, input_target_dir, boris, boris_target_dir, research_center)\n",
    "traverse()"
   ],
   "execution_count": 48,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reset_index'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 49\u001B[0m\n\u001B[0;32m     47\u001B[0m                                 \u001B[38;5;66;03m# Create one vector of biosignals, sliced BORIS and sliced video\u001B[39;00m\n\u001B[0;32m     48\u001B[0m                                 slice_vectors(video_path, exact_signals_path, input_target_dir, boris, boris_target_dir, research_center)\n\u001B[1;32m---> 49\u001B[0m \u001B[43mtraverse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[48], line 48\u001B[0m, in \u001B[0;36mtraverse\u001B[1;34m()\u001B[0m\n\u001B[0;32m     46\u001B[0m boris \u001B[38;5;241m=\u001B[39m get_boris_vector(boris_path)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# Create one vector of biosignals, sliced BORIS and sliced video\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m \u001B[43mslice_vectors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexact_signals_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_target_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mboris\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mboris_target_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresearch_center\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[47], line 63\u001B[0m, in \u001B[0;36mslice_vectors\u001B[1;34m(video_path, biosignal_path, input_storage_path, boris, boris_storage_path, research_center)\u001B[0m\n\u001B[0;32m     60\u001B[0m sliced_frame_embeddings \u001B[38;5;241m=\u001B[39m frame_embeddings[video_first_index:video_last_index]\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# Create one df with all biosignals\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([sliced_EDA\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), sliced_TEMP\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), sliced_HR\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), \u001B[43msliced_frame_embeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_index\u001B[49m()], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# Rename column names\u001B[39;00m\n\u001B[0;32m     66\u001B[0m new_columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEDA\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTEMP\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHR\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns[\u001B[38;5;241m3\u001B[39m:]\u001B[38;5;241m.\u001B[39mtolist()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'reset_index'"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
